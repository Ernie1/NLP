{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张吉祺 16340286\n",
    "\n",
    "**We want to build a naïve bayes sentiment classifier using add -1 smoothing, as described in the lecture (not binary naïve bayes, regular naïve bayes). Here is our training corpus:**\n",
    "\n",
    "**Training Set:**\n",
    "\n",
    "\\- just plain boring  \n",
    "\\- entirely predictable and lacks energy  \n",
    "\\- no surprises and very few laughs  \n",
    "\\+ very powerful  \n",
    "\\+ the most fun film of the summer\n",
    "\n",
    "**Test Set:**\n",
    "\n",
    "predictable with no originality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = [\n",
    "    'just plain boring',\n",
    "    'entirely predictable and lacks energy',\n",
    "    'no surprises and very few laughs'\n",
    "]\n",
    "train_pos = [\n",
    "    'very powerful',\n",
    "    'the most fun film of the summer'\n",
    "]\n",
    "test = 'predictable with no originality'\n",
    "\n",
    "V = set()\n",
    "def mk_dict_and_cnt(sentences):\n",
    "    res = {}\n",
    "    cnt = 0\n",
    "    for i in sentences:\n",
    "        for j in i.split():\n",
    "            if j in res:\n",
    "                res[j] += 1\n",
    "            else:\n",
    "                res[j] = 1\n",
    "            cnt += 1\n",
    "            V.add(j)\n",
    "    return res, cnt\n",
    "\n",
    "neg_dict, neg_cnt = mk_dict_and_cnt(train_neg)\n",
    "pos_dict, pos_cnt = mk_dict_and_cnt(train_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Compute the prior for the two classes + and -, and the likelihoods for each word given the class (leave in the form of fractions).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(-)\t\t= 3/5\n",
      "P(+)\t\t= 2/5\n",
      "P('just'|-)\t= 2/34\n",
      "P('plain'|-)\t= 2/34\n",
      "P('boring'|-)\t= 2/34\n",
      "P('entirely'|-)\t= 2/34\n",
      "P('predictable'|-)\t= 2/34\n",
      "P('and'|-)\t= 3/34\n",
      "P('lacks'|-)\t= 2/34\n",
      "P('energy'|-)\t= 2/34\n",
      "P('no'|-)\t= 2/34\n",
      "P('surprises'|-)\t= 2/34\n",
      "P('very'|-)\t= 2/34\n",
      "P('few'|-)\t= 2/34\n",
      "P('laughs'|-)\t= 2/34\n",
      "P(word not in -|-)\t= 1/34\n",
      "P('very'|+)\t= 2/29\n",
      "P('powerful'|+)\t= 2/29\n",
      "P('the'|+)\t= 3/29\n",
      "P('most'|+)\t= 2/29\n",
      "P('fun'|+)\t= 2/29\n",
      "P('film'|+)\t= 2/29\n",
      "P('of'|+)\t= 2/29\n",
      "P('summer'|+)\t= 2/29\n",
      "P(word not in +|+)\t= 1/29\n"
     ]
    }
   ],
   "source": [
    "V_size = len(V)\n",
    "neg_pri = len(train_neg) / (len(train_neg) + len(train_pos))\n",
    "print('P(-)\\t\\t= {}/{}'.format(len(train_neg), (len(train_neg) + len(train_pos))))\n",
    "pos_pri = len(train_pos) / (len(train_neg) + len(train_pos))\n",
    "print('P(+)\\t\\t= {}/{}'.format(len(train_pos), (len(train_neg) + len(train_pos))))\n",
    "for k, v in neg_dict.items():\n",
    "    print('P(\\'{}\\'|-)\\t= {}/{}'.format(k, v + 1, neg_cnt + V_size))\n",
    "print('P(word not in -|-)\\t= {}/{}'.format(1, neg_cnt + V_size))\n",
    "for k, v in pos_dict.items():\n",
    "    print('P(\\'{}\\'|+)\\t= {}/{}'.format(k, v + 1, pos_cnt + V_size))\n",
    "print('P(word not in +|+)\\t= {}/{}'.format(1, pos_cnt + V_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Then compute whether the sentence in the test set is of class positive or negative (you may need a computer for this final computation).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(-|\"predictable with no originality\")\t= 0.0020761245674740486\n",
      "P(+|\"predictable with no originality\")\t= 0.0004756242568370987\n",
      "P(-|\"predictable with no originality\") is greater, so the test set sentence is classified as class negative.\n"
     ]
    }
   ],
   "source": [
    "def posterior(class_prior, class_dict, class_cnt, sentence):\n",
    "    words = sentence.split()\n",
    "    res = class_prior\n",
    "    for i in words:\n",
    "        if i in V:\n",
    "            p = class_dict[i] if i in class_dict else 0\n",
    "            res *= (p + 1) / (class_cnt + V_size)\n",
    "    return res\n",
    "\n",
    "neg_post = posterior(neg_pri, neg_dict, neg_cnt, test)\n",
    "print('P(-|\\\"{}\\\")\\t='.format(test), neg_post)\n",
    "pos_post = posterior(pos_pri, pos_dict, pos_cnt, test)\n",
    "print('P(+|\\\"{}\\\")\\t='.format(test), pos_post)\n",
    "if neg_post > pos_post:\n",
    "    print('P(-|\\\"{}\\\") is greater, so the test set sentence is classified as class {}.'.format(test, 'negative'))\n",
    "elif neg_post < pos_post:\n",
    "    print('P(+|\\\"{}\\\") is greater, so the test set sentence is classified as class {}.'.format(test, 'positive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Would using binary multinomial Naïve Bayes change anything?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using binary Naïve Bayes:\n",
      "P(-|\"predictable with no originality\")\t= 0.0022038567493112946\n",
      "P(+|\"predictable with no originality\")\t= 0.000510204081632653\n",
      "P(-|\"predictable with no originality\") is greater, so the test set sentence is classified as class negative.\n"
     ]
    }
   ],
   "source": [
    "def posterior_bin(class_prior, class_dict, class_cnt, sentence):\n",
    "    words = sentence.split()\n",
    "    res = class_prior\n",
    "    for i in words:\n",
    "        if i in V:\n",
    "            p = 1 if i in class_dict else 0\n",
    "            res *= (p + 1) / (class_cnt + V_size)\n",
    "    return res\n",
    "\n",
    "print('Using binary Naïve Bayes:')\n",
    "neg_post = posterior_bin(neg_pri, neg_dict, len(neg_dict), test)\n",
    "print('P(-|\\\"{}\\\")\\t='.format(test), neg_post)\n",
    "pos_post = posterior_bin(pos_pri, pos_dict, len(pos_dict), test)\n",
    "print('P(+|\\\"{}\\\")\\t='.format(test), pos_post)\n",
    "if neg_post > pos_post:\n",
    "    print('P(-|\\\"{}\\\") is greater, so the test set sentence is classified as class {}.'.format(test, 'negative'))\n",
    "elif neg_post < pos_post:\n",
    "    print('P(+|\\\"{}\\\") is greater, so the test set sentence is classified as class {}.'.format(test, 'positive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Why do you add |𝑉| to the denominator of add-1 smoothing, instead of just counting the words in one class?**\n",
    "\n",
    "In add-1 smoothing we assume we have seen each word once regardless of whether they appear in the original class or not and thus add |𝑉| to the denominator. Note that words that do not appear in the train set are 'unk' and are not included in the vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What would the answer to question 2 be without add-1 smoothing?**\n",
    "\n",
    "P(c|\"predictable with no originality\") = 0 for class positive because at least one of the words in the test set does not appear in the positive train examples; P('predictable'|+) = P('no'|+) = 0. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
